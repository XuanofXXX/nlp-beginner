{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as npa\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import jieba\n",
    "import pickle as pkl\n",
    "from typing import Any\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "from TorchCRF import CRF\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('renmindata.pkl', 'rb') as f:\n",
    "    data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    text = \"这是一个测试文本。\" if text is None else text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"qgyd2021/chinese_ner_sft\", \"Bank\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = ds['train']['text'][0]\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "text_sample = text_sample[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, tag_size) -> None:\n",
    "        super().__init__()\n",
    "        self.tag_size = tag_size\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size //2, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_size, tag_size)\n",
    "        self.trans = nn.Parameter(torch.rand((tag_size, tag_size)))\n",
    "        \n",
    "    def _get_lstm(self, x):\n",
    "        x = self.embed(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return lstm_out\n",
    "    \n",
    "    def _forward_alg(self, feats):\n",
    "        init_alpha = torch.full((1, self.tag_size), -1000)\n",
    "        init_alpha[0][0] = 0.0\n",
    "        forward_var = init_alpha\n",
    "        batch_size = feats.shape[0]\n",
    "        for word_idx in range(feats.shape[1]):\n",
    "            feat = feats[:, word_idx, :]\n",
    "            scores = (\n",
    "                forward_var.unsqueeze(1).expand(batch_size, self.tag_size, self.tag_size) +\n",
    "                self.trans.unsqueeze(0).expand(batch_size, self.tag_size, self.tag_size) + \n",
    "                feat.unsqueeze(2).expand(batch_size, self.tag_size, self.tag_size)\n",
    "            )\n",
    "            forward_var = torch.logsumexp(scores, dim=2)\n",
    "        \n",
    "        return forward_var\n",
    "    \n",
    "    def _score_sentence(self, feats, tags):\n",
    "        score = torch.zeros(1)\n",
    "        tags = torch.cat([torch.tensor([0], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + self.trans[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        return score\n",
    "    \n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        init_vvars = torch.full((1, self.tag_size), -1000.0)\n",
    "        init_vvars[0][0] = 0.0\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []\n",
    "            viterbivars_t = []\n",
    "            for next_tag in range(self.tag_size):\n",
    "                next_tag_var = forward_var + self.trans[next_tag]\n",
    "                best_tag_id = torch.argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "        \n",
    "        best_tag_id = torch.argmax(forward_var)\n",
    "        path_score = forward_var[0][best_tag_id]\n",
    "        \n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        best_path.pop()\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "    \n",
    "    def neg_log_likelihood(self, x, tags):\n",
    "        feats = self._get_lstm(x)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feats = self._get_lstm(x)\n",
    "        score, tag_seq = self._viterbi_decode(feats)\n",
    "        return score, tag_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['text'][0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_idx': [0, 12, 19, 42, 54, 58, 64, 70],\n",
       " 'end_idx': [2, 14, 21, 44, 56, 60, 66, 71],\n",
       " 'entity_text': ['交行', '提额', '降到', '消费', '增加', '提额', '分期', '降'],\n",
       " 'entity_label': ['BANK',\n",
       "  'COMMENTS_N',\n",
       "  'COMMENTS_ADJ',\n",
       "  'COMMENTS_N',\n",
       "  'COMMENTS_N',\n",
       "  'COMMENTS_N',\n",
       "  'PRODUCT',\n",
       "  'COMMENTS_ADJ'],\n",
       " 'entity_names': [['银行', '银行名称'],\n",
       "  ['金融名词'],\n",
       "  ['形容词'],\n",
       "  ['金融名词'],\n",
       "  ['金融名词'],\n",
       "  ['金融名词'],\n",
       "  ['产品', '产品名称', '金融名词', '金融产品', '银行产品'],\n",
       "  ['形容词']]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train']['entities'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mapping_idx(text, entity):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    entity_idx = 0\n",
    "    label_list = ['O'] * len(tokens)\n",
    "    map_index = tokenizer(text, return_offsets_mapping=True)['offset_mapping']\n",
    "    map_index.pop(0)\n",
    "    map_index.pop(-1)\n",
    "    \n",
    "    for token_idx, token_int in enumerate(map_index):\n",
    "        char_start, char_end = token_int\n",
    "        if entity_idx >= len(entity['start_idx']):\n",
    "            print(token_idx)\n",
    "            break\n",
    "        try:\n",
    "            entity_start, entity_end = entity['start_idx'][entity_idx], entity['end_idx'][entity_idx]\n",
    "            label = entity['entity_label'][entity_idx]\n",
    "        except:\n",
    "            # print(tokens, entity['entity_label'])\n",
    "            print(entity_idx)\n",
    "            print(token_idx, token_int)\n",
    "            print(entity['start_idx'])\n",
    "        if char_start == entity_start:\n",
    "            label_list[token_idx] = f'B-{label}'\n",
    "        elif char_start > entity_start and char_end <= entity_end:\n",
    "            label_list[token_idx] = f'I-{label}'\n",
    "        \n",
    "        if char_end >= entity_end:\n",
    "            entity_idx += 1\n",
    "    return tokens, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['单',\n",
       "  '标',\n",
       "  '我',\n",
       "  '有',\n",
       "  '了',\n",
       "  '，',\n",
       "  '最',\n",
       "  '近',\n",
       "  'visa',\n",
       "  '双',\n",
       "  '标',\n",
       "  '返',\n",
       "  '现',\n",
       "  '活',\n",
       "  '动',\n",
       "  '好'],\n",
       " ['B-PRODUCT',\n",
       "  'I-PRODUCT',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-PRODUCT',\n",
       "  'B-PRODUCT',\n",
       "  'I-PRODUCT',\n",
       "  'B-COMMENTS_N',\n",
       "  'I-COMMENTS_N',\n",
       "  'I-COMMENTS_N',\n",
       "  'I-COMMENTS_N',\n",
       "  'B-COMMENTS_ADJ'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "text_sample = ds['train']['text'][1]\n",
    "entity = ds['train']['entities'][1]\n",
    "_mapping_idx(text_sample, entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_idx': [0, 8, 12, 14, 18],\n",
       " 'end_idx': [2, 12, 14, 18, 19],\n",
       " 'entity_text': ['单标', 'visa', '双标', '返现活动', '好'],\n",
       " 'entity_label': ['PRODUCT',\n",
       "  'PRODUCT',\n",
       "  'PRODUCT',\n",
       "  'COMMENTS_N',\n",
       "  'COMMENTS_ADJ'],\n",
       " 'entity_names': [['产品', '产品名称', '金融名词', '金融产品', '银行产品'],\n",
       "  ['产品', '产品名称', '金融名词', '金融产品', '银行产品'],\n",
       "  ['产品', '产品名称', '金融名词', '金融产品', '银行产品'],\n",
       "  ['金融名词'],\n",
       "  ['形容词']]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityData(Dataset):\n",
    "    def __init__(self, data, tokenizer) -> None:\n",
    "        super().__init__()\n",
    "        self.labels = None\n",
    "        self.token_list = None\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag_size = None\n",
    "        self.PAD_TAG = 'PAD'\n",
    "        \n",
    "        raw_text = data['text']\n",
    "        entities = data['entities']\n",
    "        self._get_CRF_labels(entities)\n",
    "        self._processing_data(raw_text, entities)\n",
    "        self.tag_size = len(self.labels2id)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_list)\n",
    "    \n",
    "    def _get_CRF_labels(self, entities):\n",
    "        raw_labels = {label for entity in entities for label in entity['entity_label']}\n",
    "        labels_cate = {f'B-{label}' for label in raw_labels} | {f'I-{label}' for label in raw_labels} | {'O'}\n",
    "        \n",
    "        self.labels2id = {label: idx+1 for idx, label in enumerate(labels_cate)}\n",
    "        self.labels2id['[PAD]'] = 0\n",
    "        self.id2labels = {idx: label for label, idx in self.labels2id.items()}\n",
    "    \n",
    "    def _mapping_idx(self, text, entity):\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        entity_idx = 0\n",
    "        label_list = ['O'] * len(tokens)\n",
    "        map_index = self.tokenizer(text, return_offsets_mapping=True)['offset_mapping']\n",
    "        map_index.pop(0)\n",
    "        map_index.pop(-1)\n",
    "        \n",
    "        for token_idx, token_int in enumerate(map_index):\n",
    "            char_start, char_end = token_int\n",
    "            if entity_idx >= len(entity['start_idx']):\n",
    "                # print(token_idx)\n",
    "                break\n",
    "            # try:\n",
    "            entity_start, entity_end = entity['start_idx'][entity_idx], entity['end_idx'][entity_idx]\n",
    "            # except:\n",
    "            #     print(entity_idx)\n",
    "            #     print(token_idx, token_int)\n",
    "            #     print(entity['start_idx'])\n",
    "            label = entity['entity_label'][entity_idx]\n",
    "            if char_start == entity_start:\n",
    "                label_list[token_idx] = f'B-{label}'\n",
    "            elif char_start > entity_start and char_end <= entity_end:\n",
    "                label_list[token_idx] = f'I-{label}'\n",
    "            if char_end >= entity_end:\n",
    "                entity_idx += 1\n",
    "        return tokens, label_list\n",
    "    \n",
    "    def _processing_data(self, raw_text, entities):\n",
    "        self.token_list = []\n",
    "        self.labels = []\n",
    "        for text, entity in zip(raw_text, entities):\n",
    "            tokens, label_list = self._mapping_idx(text, entity)\n",
    "            self.token_list.append(tokens)\n",
    "            self.labels.append(label_list)\n",
    "            \n",
    "    def decode_label(self, labels):\n",
    "        return [self.id2labels(label) for label in labels]\n",
    "    \n",
    "    def decode_text(self, token_ids):\n",
    "        return self.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        inputs_id = self.tokenizer.convert_tokens_to_ids(self.token_list[index])\n",
    "        inputs_labels = [self.labels2id[label] for label in self.labels[index]]\n",
    "        return torch.tensor(inputs_id, dtype=torch.long), torch.tensor(inputs_labels, dtype=torch.long)\n",
    "        # return {\n",
    "        #     \"input_id\": torch.tensor(inputs_id, dtype=torch.long), \n",
    "        #     \"label\": torch.tensor(inputs_labels, dtype=torch.long)\n",
    "        #     }\n",
    "        # return torch.tensor(inputs_id, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EntityData(ds['train'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    max_length = 0\n",
    "    # print('in collate_fn', max_length)\n",
    "    # print(batch)\n",
    "    for sample in batch:\n",
    "        tokens, label = sample\n",
    "        # tokens = sample['input_id']\n",
    "        # label = sample['label']\n",
    "        max_length = max(len(tokens), max_length)\n",
    "        # print(batch)\n",
    "        assert len(tokens) == len(label), f'the length of tokens {len(tokens)} is not equal to the length of labels {len(label)}'\n",
    "    \n",
    "    padded_tokens = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    for tokens, labels in batch:\n",
    "        padded_tokens.append(torch.cat([tokens, torch.zeros(max_length - len(tokens), dtype=torch.long)]))\n",
    "        padded_labels.append(torch.cat([labels, torch.zeros(max_length - len(labels), dtype=torch.long)]))\n",
    "    \n",
    "    return torch.stack(padded_tokens), torch.stack(padded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, tag_size) -> None:\n",
    "        super().__init__()\n",
    "        self.tag_size = tag_size\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size // 2, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_size, tag_size)\n",
    "        self.crf = CRF(self.tag_size)\n",
    "\n",
    "    def _get_lstm_features(self, x):\n",
    "        x = self.embed(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # print(f\"after lstm: {lstm_out.shape}\")\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "    \n",
    "    def forward(self, x, tags=None):\n",
    "        mask = (x != 0).type(torch.bool)\n",
    "        lstm_feats = self._get_lstm_features(x)\n",
    "        # print(\"LSTM feats shape: \", lstm_feats.shape)\n",
    "\n",
    "        if tags is not None:\n",
    "            # print(\"Input shape: \", x.shape)\n",
    "            # print(\"Mask shape: \", mask.shape)\n",
    "            # print(\"tags shape: \", tags.shape)\n",
    "            assert mask.shape == tags.shape\n",
    "            loss = -self.crf.forward(lstm_feats, tags, mask=mask)\n",
    "            return loss\n",
    "        else:\n",
    "            # print(\"LSTM feats shape: \", lstm_feats.shape)\n",
    "            if len(lstm_feats.shape) == 2:\n",
    "                lstm_feats.unsqueeze(0)\n",
    "            pred_tags = self.crf.viterbi_decode(lstm_feats, mask=mask)\n",
    "            return pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from TorchCRF import CRF\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 2\n",
    "sequence_size = 3\n",
    "num_labels = 5\n",
    "mask = torch.BoolTensor([[1, 1, 1], [1, 1, 0]]).to(device) # (batch_size. sequence_size)\n",
    "labels = torch.LongTensor([[0, 2, 3], [1, 4, 1]]).to(device)  # (batch_size, sequence_size)\n",
    "hidden = torch.randn((batch_size, sequence_size, num_labels), requires_grad=True).to(device)\n",
    "crf = CRF(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.9362, -2.4696], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.forward(hidden, labels, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:28<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 16.95537757873535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:26<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 14.407644271850586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:22<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 12.117145538330078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:28<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 9.773664474487305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:27<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 6.583451747894287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:26<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 5.128957271575928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:26<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 3.8844590187072754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:28<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 3.4639732837677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:28<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 2.5577492713928223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:28<00:00,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 loss: 2.573702096939087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.vocab)\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "tag_size = dataset.tag_size\n",
    "\n",
    "model = LSTM_CRF(vocab_size=vocab_size, hidden_size=hidden_dim, num_layers=2, tag_size=tag_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    epoch_loss = 0.\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # print(batch)\n",
    "        # input_ids = batch['input_ids']\n",
    "        # labels = batch['labels']\n",
    "        input_ids, labels = batch\n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss = model(input_ids, labels)\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "    loss_list.append(epoch_loss)\n",
    "\n",
    "torch.save(model.state_dict(), 'task4_.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_pth_files(directory):\n",
    "    # 列出目录中的所有文件和子目录\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # 检查每个文件是否以.pth结尾\n",
    "        for file in files:\n",
    "            if file.endswith('.pth'):\n",
    "                return True\n",
    "\n",
    "def calculate_prediction(true_label, pred_label):\n",
    "    pad_size = true_label.shape[1]\n",
    "    pred = []\n",
    "    for tl, pl in zip(true_label, pred_label):\n",
    "        seq_len = len(pl)\n",
    "        pl = torch.tensor(pl, dtype=torch.long)\n",
    "        # print(tl[:seq_len] == pl)\n",
    "        pred.append((tl[:seq_len] == pl).float().mean().item()) \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_for_pth_files('./'):\n",
    "    model = LSTM_CRF(vocab_size=vocab_size, hidden_size=hidden_dim, num_layers=2, tag_size=tag_size)\n",
    "    model.load_state_dict(torch.load('task4.pth'))\n",
    "\n",
    "prediction = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_dataloader:\n",
    "        test_input_ids, test_label = test_batch\n",
    "        pred_label = model(test_input_ids)\n",
    "        # prediction = model.decode(test_feats, test_mask)\n",
    "        pred = calculate_prediction(test_label, pred_label)\n",
    "        prediction += pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9871)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.tensor(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
